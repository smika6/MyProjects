{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbJQQKL6LBmg"
   },
   "source": [
    "# Cleaning the Articles\n",
    "- *Author*: Juan Cabanela\n",
    "- *Start Date*: December 2, 2021\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Requires the following python libraries:\n",
    "- pandas\n",
    "- numpy\n",
    "\n",
    "This script will process the articles through the same cleaning and TF-IDF vectorization as the original data, saving the results for use by others.\n",
    "\n",
    "## History\n",
    "**December 2, 2021**: Initial version of cleaning code.  It dealt with a couple (minor) issues in how `articles.csv` was structured, notably that some of the strings were stored as bytestrings and misinterpreted when read in from csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6HydbbWwBDcm",
    "outputId": "a1f40379-bd20-4b2d-e9d4-de4a9cdcb731"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pathlib\n",
    "import re\n",
    "import ast\n",
    "\n",
    "##\n",
    "## Define functions\n",
    "##\n",
    "\n",
    "\n",
    "def content_cleaner(row):\n",
    "    # Processes the row content through the cleaner\n",
    "    content = row.content\n",
    "    return string_cleaner(content)\n",
    "\n",
    "\n",
    "def title_cleaner(row):\n",
    "    # Processes the row content through the cleaner\n",
    "    title = row.title\n",
    "    return string_cleaner(str(title)) # This became necesary because some titles ended up as floats!?!\n",
    "\n",
    "\n",
    "def string_cleaner(stuff):\n",
    "    # This function takes the input string and removes line feed and space runs\n",
    "\n",
    "    # Remove line feeds and space runs\n",
    "    stuff = stuff.replace('\\n',' ')\n",
    "    stuff = re.sub(r\"\\s+\", \" \", stuff)  # Remove multiple space runs\n",
    "\n",
    "\t# Remove last word since it is likely to be a partial word anyway\n",
    "    last_space_idx = stuff.rfind(\" \")\n",
    "    stuff = stuff[:last_space_idx]\n",
    "    return stuff.strip()\n",
    "\n",
    "\n",
    "def content_second_scrub(row):\n",
    "    # Processes the row content through stop word remover\n",
    "    content = row.content\n",
    "    return second_scrub(content)\n",
    "\n",
    "\n",
    "def title_second_scrub(row):\n",
    "    # Processes the row content through stop word remover\n",
    "    title = row.title\n",
    "    return second_scrub(str(title)) # This became necesary because some titles ended up as floats!?!\n",
    "\n",
    "\n",
    "def second_scrub(stuff):\n",
    "    # Remove stop words and punctuation and make entire text lowercase\n",
    "    stuff = stuff.lower()\n",
    "    stuff = ''.join(filter(lambda c: c not in punctuation, stuff))\n",
    "\n",
    "    # Remove Stop Words\n",
    "    newstuff = \"\"\n",
    "    for word in stuff.strip().split(\" \"):\n",
    "        if word not in ENGLISH_STOP_WORDS:\n",
    "            newstuff += f\"{word} \"\n",
    "    del stuff  # Release memory (just in case)\n",
    "    return newstuff.strip()\n",
    "\n",
    "\n",
    "def bytestring_cleaner(row):\n",
    "    \"\"\" Cleans up bytestrings stored as strings in CSV, converting line feeds from Windows to \n",
    "        Unix linefeeds.\n",
    "    \"\"\"\n",
    "\n",
    "    # Processes the row content through the bytestring cleaner\n",
    "    content = row.content\n",
    "    content = parse_bytes(content)\n",
    "\n",
    "    return content.replace(\"\\r\\n\", \"\\n\")\n",
    "\n",
    "\n",
    "def parse_bytes(field):\n",
    "    \"\"\" Convert string represented in Python byte-string literal b'' syntax into\n",
    "        a decoded character string - otherwise return it unchanged.\n",
    "\n",
    "        Grabbed from https://stackoverflow.com/questions/47741235/how-to-read-bytes-object-from-csv\n",
    "    \"\"\"\n",
    "    result = field\n",
    "    try:\n",
    "        result = ast.literal_eval(field)\n",
    "    finally:\n",
    "        return result.decode() if isinstance(result, bytes) else field\n",
    "\n",
    "##\n",
    "## Define constants\n",
    "##\n",
    "DEBUG = False\n",
    "\n",
    "# List of English stopwords (grabbed from https://gist.github.com/ethen8181/d57e762f81aa643744c2ffba5688d33a and used in scikit-learn\n",
    "# and nltk)\n",
    "ENGLISH_STOP_WORDS=['a','about','above','across','after','afterwards','again','against',\n",
    "\t'ain','all','almost','alone','along','already','also','although','always','am',\n",
    "\t'among','amongst','amoungst','amount','an','and','another','any','anyhow',\n",
    "\t'anyone','anything','anyway','anywhere','are','aren','around','as','at','back',\n",
    "\t'be','became','because','become','becomes','becoming','been','before','beforehand',\n",
    "\t'behind','being','below','beside','besides','between','beyond','bill','both',\n",
    "\t'bottom','but','by','call','can','cannot','cant','co','con','could','couldn',\n",
    "\t'couldnt','cry','d','de','describe','detail','did','didn','do','does','doesn',\n",
    "\t'doing','don','done','down','due','during','each','eg','eight','either','eleven',\n",
    "\t'else','elsewhere','empty','enough','etc','even','ever','every','everyone',\n",
    "\t'everything','everywhere','except','few','fifteen','fify','fill','find','fire',\n",
    "\t'first','five','for','former','formerly','forty','found','four','from','front',\n",
    "\t'full','further','get','give','go','had','hadn','has','hasn','hasnt','have',\n",
    "\t'haven','having','he','hence','her','here','hereafter','hereby','herein','hereupon',\n",
    "\t'hers','herself','him','himself','his','how','however','hundred','i','ie','if','in',\n",
    "\t'inc','indeed','interest','into','is','isn','it','its','itself','just','keep','last',\n",
    "\t'latter','latterly','least','less','ll','ltd','m','ma','made','many','may','me',\n",
    "\t'meanwhile','might','mightn','mill','mine','more','moreover','most','mostly','move',\n",
    "\t'much','must','mustn','my','myself','name','namely','needn','neither','never',\n",
    "\t'nevertheless','next','nine','no','nobody','none','noone','nor','not','nothing',\n",
    "\t'now','nowhere','o','of','off','often','on','once','one','only','onto','or','other',\n",
    "\t'others','otherwise','our','ours','ourselves','out','over','own','part','per',\n",
    "\t'perhaps','please','put','rather','re','s','same','see','seem','seemed','seeming',\n",
    "\t'seems','serious','several','shan','she','should','shouldn','show','side','since',\n",
    "\t'sincere','six','sixty','so','some','somehow','someone','something','sometime',\n",
    "\t'sometimes','somewhere','still','such','system','t','take','ten','than','that',\n",
    "\t'the','their','theirs','them','themselves','then','thence','there','thereafter',\n",
    "\t'thereby','therefore','therein','thereupon','these','they','thick','thin','third',\n",
    "\t'this','those','though','three','through','throughout','thru','thus','to',\n",
    "\t'together','too','top','toward','towards','twelve','twenty','two','un','under',\n",
    "\t'until','up','upon','us','ve','very','via','was','wasn','we','well','were',\n",
    "\t'weren','what','whatever','when','whence','whenever','where','whereafter',\n",
    "\t'whereas','whereby','wherein','whereupon','wherever','whether','which','while',\n",
    "\t'whither','who','whoever','whole','whom','whose','why','will','with','within',\n",
    "\t'without','won','would','wouldn','y','yet','you','your','yours','yourself',\n",
    "\t'yourselves']\n",
    "\n",
    "# Define punctuation to purge\n",
    "punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~‘’–“”'\n",
    "\n",
    "# Directory containing chunked data\n",
    "data_dir = \"./\"\n",
    "articles_csv = f\"{data_dir}articles.csv\"\n",
    "var_dir = \"./Variables/\"\n",
    "cols_to_dump = ['domain', 'story_label', 'title', 'content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ./articles_fullcontext_cleaned.csv ... \n"
     ]
    }
   ],
   "source": [
    "# Load the target articles from the CSV file\n",
    "articles_df = pd.read_csv(articles_csv)\n",
    "\n",
    "# Change column names\n",
    "col_dict = {\"Source Name\": \"domain\", \"Related Story\": \"story_label\", \"Title\":\"title\", \"Text\":\"content\", \"URL\":\"url\"}\n",
    "articles_df.rename(columns=col_dict, inplace=True)\n",
    "\n",
    "# Make source names into proper domain names\n",
    "domain_map = {'CNN':'cnn.com', 'Fox News':'foxnews.com', 'NBC':'nbcnews.com', 'New York Post':'nypost.com', 'The Wall Street Journal':'wsj.com'}\n",
    "articles_df['domain'].replace(to_replace=domain_map,inplace=True)\n",
    "\n",
    "#Convert mal-stored bytestrings to proper strings\n",
    "articles_df[\"content\"] = articles_df.apply(bytestring_cleaner, axis=1)\n",
    "\n",
    "# Reduce content to first 800 characters\n",
    "articles_df[\"content\"] = articles_df[\"content\"].str[:800]\n",
    "# clean all the article content\n",
    "articles_df['content'] = articles_df.apply(content_cleaner, axis=1)\n",
    "# clean all the title content\n",
    "articles_df['title'] = articles_df.apply(title_cleaner, axis=1)\n",
    "\n",
    "# Dump articles with context before stripping the stop words (and thus wiping context)\n",
    "fname_fullcontext_cleaned = f\"{data_dir}articles_fullcontext_cleaned.csv\"\n",
    "print(f\"Creating {fname_fullcontext_cleaned} ... \")\n",
    "cols_to_dump = ['domain', 'story_label', 'title', 'content']\n",
    "articles_df[cols_to_dump].to_csv(fname_fullcontext_cleaned, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ./articles_cleaned.csv ... "
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a separate cleaned pandas dataframe\n",
    "cleaned_df = articles_df.copy(deep=True)\n",
    "# clean all the article stopwords, remove punctuation, and convert to lowercase\n",
    "cleaned_df['content'] = cleaned_df.apply(content_second_scrub, axis=1)\n",
    "# clean all the title stopwords, remove punctuation, and convert to lowercase\n",
    "cleaned_df['title'] = cleaned_df.apply(title_second_scrub, axis=1)\n",
    "\n",
    "fname_cleaned = f\"{data_dir}articles_cleaned.csv\"\n",
    "print(f\"Creating {fname_cleaned} ... \", end='')\n",
    "cleaned_df[cols_to_dump].to_csv(fname_cleaned, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply TF-IDF Vectorizer that was used with FakeNewsCorpus\n",
    "\n",
    "We will read in an apply the same TF-IDF vectorizer that was developed with the FakeNewsCorpus (in `ProjectDataTFIDFTokenization.ipynb` notebook).  Apply it to the complete full_context article data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously pickled vectorizer (about 0.02 GB)\n",
      "Creating ./Variables/articles_fullcontext_tokenized_100000.p ... \n"
     ]
    }
   ],
   "source": [
    "# Number of words kept by TF-IDF tokenizer\n",
    "words2keep = 100000\n",
    "\n",
    "# Name of articles tokenized content file\n",
    "tokenized_pickle_file = f\"{var_dir}articles_fullcontext_tokenized_{words2keep:06d}.p\"\n",
    "\n",
    "# Set up path to pickle file of the vectorizer\n",
    "vectorizer_pickle_file = f\"{var_dir}fullcontext_vectorizer_{words2keep:06d}.p\"\n",
    "vectorizer_pickle_path = pathlib.Path(vectorizer_pickle_file)\n",
    "# If pickle files exist, avoid reprocessing and just load them\n",
    "if (vectorizer_pickle_path.is_file()  ):\n",
    "    print(f\"Loading previously pickled vectorizer (about {vectorizer_pickle_path.stat().st_size/1024**3:0.2f} GB)\")\n",
    "    vectorizer = pickle.load( open( vectorizer_pickle_file, \"rb\" ) )\n",
    "\n",
    "    # Convert content to strings then apply transform from TFIDTVectorizer\n",
    "    corpus = articles_df['content'].apply(lambda x: np.str_(x))\n",
    "    tokenized_content = vectorizer.transform(corpus)\n",
    "\n",
    "    print(f\"Creating {tokenized_pickle_file} ... \")\n",
    "    pickle.dump( tokenized_content, open(tokenized_pickle_file, \"wb\" ) )\n",
    "else:\n",
    "    print(f\"PROCEED NO FURTHER until you have created vectorizer and pickled it as {vectorizer_pickle_file}.\")\n",
    "    print(\"- You can create the appropriate TF-IDF vectorizer using ProjectDataTFIDFtokenization.ipynb.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DataInspection412.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "12f6268eea5eaf9f8522c8e6c3e7b3815cade26c9b1340631fb84fc54a3083ce"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
