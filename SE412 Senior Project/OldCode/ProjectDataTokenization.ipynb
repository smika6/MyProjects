{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbJQQKL6LBmg"
   },
   "source": [
    "# Tokenizing the FakeNewsCorpus\n",
    "- *Author*: Juan Cabanela\n",
    "- *Start Date*: November 1, 2021\n",
    "\n",
    "### Requirements\n",
    "\n",
    "Requires the following python libraries:\n",
    "- pandas\n",
    "- numpy\n",
    "- tensorflow\n",
    "- scikit-learn (sklearn)\n",
    "\n",
    "This notebook should only be executed using the cleaned version of the FakeNewsCorpus that has been \"chunked\" into smaller portions, mostly because that is the way the data will be loaded.\n",
    "\n",
    "The `chunked_dir` defined at the end of the first block of code is where the notebook will look for the cleaned and chunked FaceNewsCorpus files (I defaulted to `./FakeNewsCorpus/news_chunked/`).\n",
    "\n",
    "This script will do the work of tokenizing the words (representing words with numbers) in the corpus.  Based on my reading I made a few decisions to save on memory use as inspired by the Chapter 16 discussion in *Hands-on Machine Learning with Scikit-Learn, Keras, & TensorFlow* by Aurélien Géron on tokenizing a text for sentiment analysis:\n",
    "\n",
    "- I will tokenize both the titles and the content.\n",
    "- I will only keep the 10,000 most common words.\n",
    "\n",
    "### History \n",
    "**November 4, 2021**: First complete version of this code is done, it takes the cleaned Corpus and tokenizes the text in the Corpus, saving the final dataframe as both a pickle file (to keep the structure intact) and as a CSV file (which would have to be parsed carefully to keep the structure intact).  This pickle file is only 2.49GB in size!\n",
    "\n",
    "**November 14, 2021**: Performed some statistics on the numbers of articles in each class. Also added boolean columns for each class since some articles can have up to three classes assigned.  I also added a TF-IDF tokenizer from Scikit-Learn to the arsenal, although I didn't save the results of running that Tokenizer, it is considerably faster (taking only 9 minutes to run).\n",
    "\n",
    "**November 15, 2021** Removed code for dealing with data that as cleaned using TF-IDF tokenization to a separate notebook.  Also had the code automatically detect if pickle files exist from previous processing to avoid re-running everyhing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6HydbbWwBDcm",
    "outputId": "a1f40379-bd20-4b2d-e9d4-de4a9cdcb731"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "##\n",
    "## Custom functions\n",
    "##\n",
    "\n",
    "# Set up tokenizing function\n",
    "def tokenize(ele):\n",
    "    # text must be lowercase string to properly tokenize it\n",
    "    # Requires table to be defined (as global) before running it)\n",
    "    return np.array(table.lookup(tf.constant(str(ele).lower().split())), dtype=np.int32)\n",
    "\n",
    "\n",
    "##\n",
    "## Define constants\n",
    "##\n",
    "\n",
    "# Directory containing chunked data\n",
    "data_dir = \"./FakeNewsCorpus/\"\n",
    "chunked_dir = f\"{data_dir}news_chunked/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the chunked data files\n",
    "\n",
    "Read the files which have already been cleaned (should be only about 3.5GB) and build a single dataframe containing the entire cleaned dataset. It takes about 50sec to load all the data into memory. The full dataset now occupies about 5.5GB of memory unpacked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up path to chunked CSV files\n",
    "chunked_path = pathlib.Path(chunked_dir)\n",
    "corpus_pickle = f\"{data_dir}cleaned_corpus.p\"\n",
    "corpus_csv = f\"{data_dir}cleaned_corpus.csv\"\n",
    "vocab_pickle = \"vocabulary.p\"\n",
    "corpus_pickle_path = pathlib.Path(corpus_pickle)\n",
    "vocab_pickle_path = pathlib.Path(vocab_pickle)\n",
    "\n",
    "# Check if the pickle files exist, if not, process everything\n",
    "if (corpus_pickle_path.is_file() & vocab_pickle_path.is_file()):\n",
    "    print(f\"Loading previously pickled final_df (about {corpus_pickle_path.stat().st_size/1024**3:0.2f} GB)\")\n",
    "    final_df = pickle.load( open( corpus_pickle, \"rb\" ) )\n",
    "    print(f\"Loading previously pickled vocabulary (about {vocab_pickle_path.stat().st_size/1024**3:0.2f} GB)\")\n",
    "    vocabulary = pickle.load( open( vocab_pickle, \"rb\" ) )\n",
    "\n",
    "    print(f\"Loaded final_df which is occupying {final_df.memory_usage(deep=True).sum()/1024**3:0.3f} GB of memory.\")\n",
    "\n",
    "    # Get all the categories as a list\n",
    "    rawcategories = np.array(final_df[\"1st_type\"].unique()).astype('str')\n",
    "    categories = rawcategories[rawcategories != 'nan'].tolist()\n",
    "else:\n",
    "    # Create master dataframe\n",
    "    master_df = pd.DataFrame(columns=[\"title\", \"content\", \"1st_type\" , \"2nd_type\", \"3rd_type\"])\n",
    "\n",
    "    # Iterate through chunked csv files alphabetically\n",
    "    n_tot = 0\n",
    "\n",
    "    print(\"Loading records: \")\n",
    "    for i, csvname in enumerate(sorted(chunked_path.iterdir())):\n",
    "        if \".csv\" in str(csvname):\n",
    "            print(f\"   {csvname.name}: \", end=\"\")\n",
    "            chunked_df = pd.read_csv(csvname, dtype={'domain': str, 'title': str, 'content': str, '1st_type': str, '2nd_type': str, '3rd_type': str})\n",
    "            n_tot = n_tot + chunked_df.shape[0]\n",
    "            master_df = master_df.append(chunked_df, ignore_index=True)\n",
    "            print(f\" {chunked_df.shape[0]} entries loaded.\")\n",
    "\n",
    "    print(f\"\\n{n_tot} entries loaded into master_df which is occupying {master_df.memory_usage(deep=True).sum()/1024**3:0.3f} GB of memory.\")\n",
    "\n",
    "    # Get all the categories as a list\n",
    "    rawcategories = np.array(master_df[\"1st_type\"].unique()).astype('str')\n",
    "    categories = rawcategories[rawcategories != 'nan'].tolist()\n",
    "\n",
    "    # Collect 1st_type counts\n",
    "    summary = master_df[\"1st_type\"].value_counts()\n",
    "\n",
    "    # Add a column for each category, flagging the entry as an example, collect stats\n",
    "    for cat in categories:\n",
    "        master_df[cat] = (master_df['1st_type'] == cat) | (master_df['2nd_type']  == cat) | (master_df['3rd_type']  == cat)\n",
    "        print(f\"- '{cat}': {len(master_df[master_df[cat]])} total articles (only {summary[cat]} as 1st_type).\")\n",
    "\n",
    "    # Build a vocabulary and save it to pickle file for later use (takes about 2min ro process the entire dataset)\n",
    "    vocabulary = Counter()\n",
    "\n",
    "    print(f\"Determining vocabulary for {n_tot} entries (expect this to take a few minutes):\")\n",
    "    for i, content in enumerate(master_df['content']):\n",
    "        # Add vocabulary (force content to be string, in case it was typed as something else)\n",
    "        vocabulary.update(str(content).strip().split(\" \"))\n",
    "        if i%250000 == 0:\n",
    "            print(f\"{i:07d}...\", end='')\n",
    "    print(\"\\nDONE!\")\n",
    "\n",
    "    # Write vocab to file to avoid having to reprocess\n",
    "    pickle.dump( vocabulary, open(vocab_pickle, \"wb\" ) )\n",
    "\n",
    "    # Check size of vocabulary\n",
    "    print (f\" There are {len(vocabulary)} words in the complete vocabulary (sans English stop words)!\")\n",
    "\n",
    "    # Let's select only the top  10,000 words\n",
    "    vocab_size = 10000\n",
    "    truncated_vocabulary = [word for word,count in vocabulary.most_common()[:vocab_size]]\n",
    "    print (f\" There are {len(truncated_vocabulary)} words in the truncated vocabulary (sans English stop words)!\")\n",
    "\n",
    "    # Set up TensorFlow for tokenizing (a la Chapter 16 of Hands-on Machine Learning with Scikit-Learn, Keras, & TensorFlow by Aurélien Géron)\n",
    "    words = tf.constant(truncated_vocabulary)\n",
    "    wordIDs = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "    vocab_init = tf.lookup.KeyValueTensorInitializer(words, wordIDs)\n",
    "    # Create a 1000 out of vocabulary buckets\n",
    "    num_OOV_buckets = 1000\n",
    "    table = tf.lookup.StaticVocabularyTable(vocab_init, num_OOV_buckets)\n",
    "\n",
    "    # Test it\n",
    "    np.array(table.lookup(tf.constant(b\"President Obama is not Trump\".lower().split()))).tolist()\n",
    "\n",
    "    # Apply the lookup to every 'title' and 'contents' and save the results (takes about 45 min to run)\n",
    "    print(f\"Tokenizing 'title' for {n_tot} entries ... \", end='')\n",
    "    master_df['title_tokens'] = master_df['title'].apply(tokenize)\n",
    "    print(f\"and 'content' for {n_tot} entries ... \", end='')\n",
    "    master_df['content_tokens'] = master_df['content'].apply(tokenize)\n",
    "    print(\" DONE!\")\n",
    "\n",
    "    # Extract only desired columns\n",
    "    final_df = master_df[['domain', '1st_type', '2nd_type', '3rd_type', 'title_tokens', 'content_tokens', 'rumor', 'hate', 'unreliable',\n",
    "                            'conspiracy', 'clickbait', 'satire', 'fake', 'bias', 'political', 'junksci', 'reliable']]\n",
    "\n",
    "    # Save the results both as pickle and as CSV\n",
    "    print(f\"Creating {picklename} ... \")\n",
    "    pickle.dump( final_df, open(corpus_pickle, \"wb\" ) )\n",
    "    print(f\"Creating {csvname} ... \")\n",
    "    final_df.to_csv(corpus_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DataInspection412.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "12f6268eea5eaf9f8522c8e6c3e7b3815cade26c9b1340631fb84fc54a3083ce"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
