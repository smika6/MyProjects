{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbJQQKL6LBmg"
   },
   "source": [
    "# Cleaning the FakeNewsCorpus\n",
    "- *Author*: Juan Cabanela\n",
    "- *Start Date*: November 1, 2021\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Requires the following python libraries:\n",
    "- pandas\n",
    "- numpy\n",
    "- shutil\n",
    "- fasttext\n",
    "\n",
    "To run `fasttext` you must install the package and download the 128MB `lid.176.bin` model datafile (`https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin`) and place it in a `fasttext/` subdirectory with this code.\n",
    "\n",
    "This notebook should only be executed after you have downloaded and assembled the FakeNewsCorpus files (https://github.com/several27/FakeNewsCorpus). You should have the following files in the `data_dir` directory:\n",
    "- news.sample.csv\n",
    "- websites.csv\n",
    "- news_cleaned_2018_02_13.csv.zip (or news_cleaned_2018_02_13.csv)\n",
    "\n",
    "The `data_dir` defined toward the end of the first block of code is where the notebook will look for the FaceNewsCorpus files (my choice was `./FakeNewsCorpus/`).\n",
    "\n",
    "## History \n",
    "**November 2, 2021**: Initial exploratory version just used the `cleanDataframe` code but made no attempt to reduce the information in the dataframe except by dropping columns and merging in `websites.csv` information.\n",
    "\n",
    "**November 3, 2021**: After further development, I moved some of the code for cleaning the data from the Tokenization notebook to this one, which includes code to reduce the `content` column to the first 800 characters (10 lines) and code to clean both the `content` and `title` to remove punctuation and capitalization to make it easier to process.  Even though this is tossing out data, the opening of a news article is often touted as where the writer has to make the most important statements, so I am hoping they will have enough information within them to determine if there are any journalistic flaws.\n",
    "\n",
    "**November 4, 2021**: Examination of some of the dropped entries revealed that the FaceNewsCorpus contained a lot of additional `reliable` (aka mainstream media) news sources that I was tossing out because there were not in the `websites.csv` file, which seems to list only sites with some sort of flaw.  I modified the merge logic to keep these sources.  Pushed up the number of articles we keep to 8.1 million!  I also removed a stupid error in the puncutation I was removing to also remove a closing quotation).\n",
    "\n",
    "The final step I performed in the cleaning was purging articles that were not in English. Turns out this is a bit unfeaasible. I tried several language detection libraries (`langdetect`, `langid` , and `fasttext`) and settled on `fasttext` as the fastest.  It turned out `langdetect` was faster, handling 250 strings in about 3 seconds to do 250 strings, but this extrapolates to about 20-25 minutes for each 250,000 'chunk' of data, so doing all the purging of English would take about 2 days!  `fastext` seems to process each chunk of 250,000 articles in about 60-80 seconds, so that is much more feasible.\n",
    "\n",
    "**November 14, 2021:** As it looked likely that we would want to have the full context of the text in some cases, I separated the removal of\n",
    "the stop words and dumped out two versions of the data files for later use.\n",
    "\n",
    "**November 24, 2021:** Fixed an issue where some of the `1st_type` entries are blank.  Error was partly due to `amazon.com` entries (wierd) and other domains not matching the websites list. `blogspot.ie` set up to match to `blogspot.com`.  However, still have the following domains (not in websites.csv) which ended up dropped (`hugedomains.com`, `21wire.tv`, `amazon.com`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6HydbbWwBDcm",
    "outputId": "a1f40379-bd20-4b2d-e9d4-de4a9cdcb731"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import re\n",
    "import fasttext\n",
    "\n",
    "##\n",
    "## Define functions\n",
    "##\n",
    "\n",
    "# Define function to create directories (wiping out existing directories, possibly dangerous!!!)\n",
    "def create_dir(dir, clobber=False, ):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "        print(\"Created Directory : \", dir)\n",
    "    else:\n",
    "        if (not clobber):\n",
    "            ans = input(f\"Should we remove the existing {dir} directory (y/n)?\")\n",
    "        else:\n",
    "            ans = 'y'\n",
    "        if (ans.lower()[0] == 'y'):\n",
    "            try:\n",
    "                shutil.rmtree(dir)\n",
    "            except OSError as e:\n",
    "                print(\"Error: %s : %s\" % (dir_path, e.strerror))\n",
    "            os.makedirs(dir)\n",
    "    return dir\n",
    "\n",
    "\n",
    "# Print information on the Pandas dataframe\n",
    "def print_FrameInfo(df):\n",
    "    print(f\"Number of records: {df.shape[0]}\")\n",
    "    print(f\"Columns: {df.columns}\")\n",
    "    print(f\"Data types: {df.dtypes}\")\n",
    "\n",
    "\n",
    "# Check for nulls in Pandas dataframe and report it\n",
    "def check_nulls(df, verbose=False):\n",
    "    if df.isnull().values.any():\n",
    "        if (verbose):\n",
    "            print(\"WARNING: You have null values somewhere in your Pandas dataframe!!!\")\n",
    "            for col in df.columns:\n",
    "                print(f\" - Found {sample_df[col].isnull().sum()} null '{col}' entries.\")\n",
    "\n",
    "    return df.isnull().values.any()\n",
    "\n",
    "\n",
    "# Function that attempts to identify the language\n",
    "# Based on code at https://ricardoanderegg.com/posts/python-fast-language-identification-fasttext/\n",
    "def lang(text):\n",
    "    # Fasttext can only handle one line at a time, so only process the first line\n",
    "    text = re.sub(r'\\n.*',r'',text)\n",
    "    # return empty string if there is no text\n",
    "    if text.isspace():\n",
    "        return \"\"\n",
    "    else:\n",
    "        # get first item of the prediction tuple, then split by \"__label__\" and return only language code\n",
    "        return lid_model.predict(text)[0][0].split(\"__label__\")[1]\n",
    "\n",
    "\n",
    "# Clean Articles dataframe by purging records with no type or unknown type and also matching up\n",
    "# website dataframe information to get additional types as needed.\n",
    "def clean_Dataframe(articles_df, classedweb_df, verbose = False):\n",
    "    # Make deep copies to work with (and to avoid triggering SettingWithCopyWarning)\n",
    "    art_df = articles_df.copy(deep=True)\n",
    "    types_df = classedweb_df.copy(deep=True)\n",
    "\n",
    "    # Convert the domains to lower case and strip out any leading \"www.\" (to allow merging with websites.csv data)\n",
    "    art_df['domain'] = art_df['domain'].str.lower()\n",
    "    art_df['domain'] = art_df['domain'].str.replace('www\\.', '', regex=True)\n",
    "\n",
    "    # Clean up alternative domain name issue with blogspot.\n",
    "    art_df['domain'] = art_df['domain'].replace(regex={r'blogspot.ie': 'blogspot.com'})\n",
    "\n",
    "    # merge website information with article information to get updated types\n",
    "    merged_df = pd.merge(art_df, types_df, on = \"domain\", how = \"left\")  # Keep all original records, but match to website where possible\n",
    "\n",
    "    # Remove \"unknown\" 1st_type\n",
    "    merged_df = merged_df[merged_df['1st_type'] != 'unknown'].copy(deep=True)\n",
    "\n",
    "    # Determine the language of the article text and remove non-English articles.\n",
    "    merged_df['lang'] = merged_df['content'].apply(lang)\n",
    "    merged_df = merged_df[merged_df['lang'] == 'en'].copy(deep=True)\n",
    "    merged_df = merged_df.drop(columns=['lang'])\n",
    "\n",
    "    # Address those cases where there was not a match in the merge above by seting '1st_type' to 'type' (and trusting the classifications).\n",
    "    noweb = merged_df['1st_type'].isna()\n",
    "    merged_df['1st_type'][noweb] = merged_df['type'][noweb]\n",
    "\n",
    "    # remove entries where '1st_type' is NaN\n",
    "    merged_df = merged_df.dropna(subset=['1st_type'])\n",
    "\n",
    "    # Reset comparison after addressing mismatched domains.\n",
    "    merged_df['compare'] = (merged_df['type'] == merged_df['1st_type'])\n",
    "    mismatch = (merged_df['compare'] == False)\n",
    "    notnull_mismatch = (merged_df['type'].notna()) & (merged_df['compare'] == False) & (merged_df['domain'] != 'patriotnewsdaily.com') & (merged_df['domain'] != 'madworldnews.com')\n",
    "    n_mismatch = merged_df[mismatch].shape[0]\n",
    "    n_mismatch_notnull = merged_df[notnull_mismatch].shape[0]\n",
    "    n_updated = n_mismatch-n_mismatch_notnull\n",
    "    if (n_mismatch_notnull > 0):\n",
    "        print(f\"WARNING: Found {n_mismatch_notnull} entires in which a non-null 'type' from sample doesn't match '1st_type' from websites.\")\n",
    "        print(merged_df[['domain', 'type','1st_type','2nd_type','3rd_type']][notnull_mismatch])\n",
    "\n",
    "    if (n_updated>0) and (verbose):\n",
    "        print(f\"NOTE: Updated types for {n_updated} entries in which 'type' was previously NaN.\")\n",
    "\n",
    "    # Drop unused columns and redundant type columns and return\n",
    "    merged_df.drop(columns={'compare', 'type'}, inplace=True)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def content_cleaner(row):\n",
    "    # Processes the row content through the cleaner\n",
    "    content = row.content\n",
    "    return string_cleaner(content)\n",
    "\n",
    "\n",
    "def title_cleaner(row):\n",
    "    # Processes the row content through the cleaner\n",
    "    title = row.title\n",
    "    return string_cleaner(str(title)) # This became necesary because some titles ended up as floats!?!\n",
    "\n",
    "\n",
    "def string_cleaner(stuff):\n",
    "    # This function takes the input string and removes line feed and space runs\n",
    "\n",
    "    # Remove line feeds and space runs\n",
    "    stuff = stuff.replace('\\n',' ')\n",
    "    stuff = re.sub(r\"\\s+\", \" \", stuff)  # Remove multiple space runs\n",
    "\n",
    "\t# Remove last word since it is likely to be a partial word anyway\n",
    "    last_space_idx = stuff.rfind(\" \")\n",
    "    stuff = stuff[:last_space_idx]\n",
    "    return stuff.strip()\n",
    "\n",
    "\n",
    "def content_second_scrub(row):\n",
    "    # Processes the row content through stop word remover\n",
    "    content = row.content\n",
    "    return second_scrub(content)\n",
    "\n",
    "\n",
    "def title_second_scrub(row):\n",
    "    # Processes the row content through stop word remover\n",
    "    title = row.title\n",
    "    return second_scrub(str(title)) # This became necesary because some titles ended up as floats!?!\n",
    "\n",
    "\n",
    "def second_scrub(stuff):\n",
    "    # Remove stop words and punctuation and make entire text lowercase\n",
    "    stuff = stuff.lower()\n",
    "    stuff = ''.join(filter(lambda c: c not in punctuation, stuff))\n",
    "\n",
    "    # Remove Stop Words\n",
    "    newstuff = \"\"\n",
    "    for word in stuff.strip().split(\" \"):\n",
    "        if word not in ENGLISH_STOP_WORDS:\n",
    "            newstuff += f\"{word} \"\n",
    "    del stuff  # Release memory (just in case)\n",
    "    return newstuff.strip()\n",
    "\n",
    "\n",
    "##\n",
    "## Define constants\n",
    "##\n",
    "\n",
    "# List of English stopwords (grabbed from https://gist.github.com/ethen8181/d57e762f81aa643744c2ffba5688d33a and used in scikit-learn\n",
    "# and nltk)\n",
    "ENGLISH_STOP_WORDS=['a','about','above','across','after','afterwards','again','against',\n",
    "\t'ain','all','almost','alone','along','already','also','although','always','am',\n",
    "\t'among','amongst','amoungst','amount','an','and','another','any','anyhow',\n",
    "\t'anyone','anything','anyway','anywhere','are','aren','around','as','at','back',\n",
    "\t'be','became','because','become','becomes','becoming','been','before','beforehand',\n",
    "\t'behind','being','below','beside','besides','between','beyond','bill','both',\n",
    "\t'bottom','but','by','call','can','cannot','cant','co','con','could','couldn',\n",
    "\t'couldnt','cry','d','de','describe','detail','did','didn','do','does','doesn',\n",
    "\t'doing','don','done','down','due','during','each','eg','eight','either','eleven',\n",
    "\t'else','elsewhere','empty','enough','etc','even','ever','every','everyone',\n",
    "\t'everything','everywhere','except','few','fifteen','fify','fill','find','fire',\n",
    "\t'first','five','for','former','formerly','forty','found','four','from','front',\n",
    "\t'full','further','get','give','go','had','hadn','has','hasn','hasnt','have',\n",
    "\t'haven','having','he','hence','her','here','hereafter','hereby','herein','hereupon',\n",
    "\t'hers','herself','him','himself','his','how','however','hundred','i','ie','if','in',\n",
    "\t'inc','indeed','interest','into','is','isn','it','its','itself','just','keep','last',\n",
    "\t'latter','latterly','least','less','ll','ltd','m','ma','made','many','may','me',\n",
    "\t'meanwhile','might','mightn','mill','mine','more','moreover','most','mostly','move',\n",
    "\t'much','must','mustn','my','myself','name','namely','needn','neither','never',\n",
    "\t'nevertheless','next','nine','no','nobody','none','noone','nor','not','nothing',\n",
    "\t'now','nowhere','o','of','off','often','on','once','one','only','onto','or','other',\n",
    "\t'others','otherwise','our','ours','ourselves','out','over','own','part','per',\n",
    "\t'perhaps','please','put','rather','re','s','same','see','seem','seemed','seeming',\n",
    "\t'seems','serious','several','shan','she','should','shouldn','show','side','since',\n",
    "\t'sincere','six','sixty','so','some','somehow','someone','something','sometime',\n",
    "\t'sometimes','somewhere','still','such','system','t','take','ten','than','that',\n",
    "\t'the','their','theirs','them','themselves','then','thence','there','thereafter',\n",
    "\t'thereby','therefore','therein','thereupon','these','they','thick','thin','third',\n",
    "\t'this','those','though','three','through','throughout','thru','thus','to',\n",
    "\t'together','too','top','toward','towards','twelve','twenty','two','un','under',\n",
    "\t'until','up','upon','us','ve','very','via','was','wasn','we','well','were',\n",
    "\t'weren','what','whatever','when','whence','whenever','where','whereafter',\n",
    "\t'whereas','whereby','wherein','whereupon','wherever','whether','which','while',\n",
    "\t'whither','who','whoever','whole','whom','whose','why','will','with','within',\n",
    "\t'without','won','would','wouldn','y','yet','you','your','yours','yourself',\n",
    "\t'yourselves']\n",
    "\n",
    "# Define punctuation to purge\n",
    "punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~‘’–“”'\n",
    "\n",
    "# Define data directory\n",
    "data_dir = \"./FakeNewsCorpus/\"\n",
    "\n",
    "# Set up language classifier model\n",
    "fasttext.FastText.eprint = lambda x: None   # Suppresses stupid warning about deprecation\n",
    "lid_model = fasttext.load_model('fasttext/lid.176.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the websites classifications from OpenSource.co for the purpose of allowing updating/correcting\n",
    "# of 'type' assignments in main corpus (including tracking all three known types).\n",
    "WebClass = f\"{data_dir}websites.csv\"\n",
    "orig_websites_df = pd.read_csv(WebClass)[['url','type','2nd_type','3rd_type']]\n",
    "\n",
    "# Convert website data to have all lowercase urls and fix column names for later\n",
    "websites_df = orig_websites_df.copy(deep=True)\n",
    "websites_df['url'] = websites_df['url'].str.lower()\n",
    "\n",
    "websites_df = websites_df.rename(columns={\"type\": \"1st_type\", \"url\": \"domain\"})  # Rename columns for easier joining\n",
    "\n",
    "# # Print information on websites csv\n",
    "# print(f\"Read in {WebClass} file and found:\")\n",
    "# print_FrameInfo(websites_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Sample Corpus (OPTIONAL)\n",
    "\n",
    "### You do NOT need to run the following cell to clean the data.\n",
    "\n",
    "I started by exploring the sample news corpus file and looking at how to reduce it down to less data and see if there were any issues.  This cell summarizes that work.  The key finding was that in some cases, the `type` column was set to `NaN` (not a number, so essentially a 'null' value).\n",
    "\n",
    "It looks like in a couple cases, the 'domain' didn't exactly match one of the URLs in the `websites.csv` file, so a 'type' was NOT assigned. I ended up fixing this by converting all the 'domain' values to be lowercase and removing any leading 'www.'.  This allowed me to perform a join with data from the websites.csv file to recover the correct 'type' (and, in fact, I match the [up to] three 'types' listed in the websites.csv file).\n",
    "\n",
    "All this is done in the `clean_Dataframe()` function.  In the process of developing that function I also discovered the guy who built the FaceNewsCorpus occasionally had some articles from websites that were not known and so the articles were typed as 'unknown.'  If I was unable to match them to the 'websites.csv' file, I dropped those articles while cleaning.  All the entries should now have at last the '1st_type' assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sample corpus\n",
    "SampleCorpus = f\"{data_dir}news_sample.csv\"\n",
    "orig_sample_df = pd.read_csv(SampleCorpus)\n",
    "\n",
    "# Subset just the part of the pandas data frame I want to keep.\n",
    "# The idea here is to JUST keep the title, article text, and tagging information.\n",
    "sample_df = orig_sample_df[['domain','type','title','content']]\n",
    "\n",
    "# Print information\n",
    "print(f\"Read in {SampleCorpus} file and found:\")\n",
    "print_FrameInfo(sample_df)\n",
    "\n",
    "# I discovered a few of the entries had NaNs, this reports they are in the type column\n",
    "if (check_nulls(sample_df, verbose=True)):\n",
    "    print(\"Well crap, you need to investigate these nulls...\")\n",
    "\n",
    "# Retrieve a cleaned dataframe\n",
    "revised_df = clean_Dataframe(sample_df, websites_df, verbose=True)\n",
    "revised_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5yOV1csVuVc"
   },
   "source": [
    "## Read and Chunk the full FakeNewsCorpus file\n",
    "\n",
    "Read the main compressed FakeNewsCorpus file in using Pandas CSV importer (which supports ZIP compressed CSV files).  We will \"chunk it\" to subfiles each of 250,000 records.  In the process I will remove bad entries with unknown types or cases where the '1st_type' is 'unknown'.\n",
    "\n",
    "### Problem discovered while processing\n",
    "It looks like a few websites were coded differently in the FakeNewsCorpus vs. the website.csv file.\n",
    "1. madworldnews.com coded as \"unreliable\" where websites.csv says \"fake\".\n",
    "2. patriotnewsdaily.com coded as \"satire\" where websites.csv says \"bias\".\n",
    "\n",
    "I coded the `clean_Dataframe()` function to ignore these cases but still look for other mismatches, no other ones found.\n",
    "\n",
    "This also runs athe first 800 character in the `content` column through `content_cleaner` which removes punctuation, newlines, and English stop words and converts the entry to lowercase.\n",
    "\n",
    "**KEY RESULT:** This routine takes about 3min30sec to run per chunk (about 3 hours total), but reduces 29GB of raw data to about 4.3GB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y3bn8miR_gQ7",
    "outputId": "cff3dc1b-ac85-4cbf-9071-b83235ad59dc"
   },
   "outputs": [],
   "source": [
    "# Activating DEBUGGING CODE\n",
    "DEBUG = 0\n",
    "\n",
    "# Determine size of original Corpus file\n",
    "OriginalCorpus = f\"{data_dir}/news_cleaned_2018_02_13.csv.zip\"\n",
    "b = os.path.getsize(OriginalCorpus)\n",
    "print(f\"FakeNewsCorpus file {OriginalCorpus} has size {b/1024**3:0.3f} GB.\")\n",
    "\n",
    "# Write out the chunked frames into separate files, but maintain the keys in the first line of the csv when writing frame to csv.\n",
    "\n",
    "# Load data using dataframes of limited length...\n",
    "chunklines = 250000  # Number of entries per chunk\n",
    "df = pd.read_csv(OriginalCorpus, iterator=True, chunksize=250000, low_memory=False, compression='zip', lineterminator='\\n')\n",
    "\n",
    "# Create directory to place chunks in\n",
    "if (DEBUG):\n",
    "    preclean_dir = f\"{data_dir}news_precleaned/\"\n",
    "    dir_created = create_dir(preclean_dir, clobber=True)\n",
    "    print(f\"Created {dir_created} directory to store precleaned datafiles (clobbering previous version).\")\n",
    "\n",
    "fullcontext_dir = f\"{data_dir}fullcontextnews_chunked/\"\n",
    "dir_created = create_dir(fullcontext_dir, clobber=True)\n",
    "print(f\"Created {dir_created} directory to store full context chunked datafiles (clobbering previous version).\")\n",
    "\n",
    "chunked_dir = f\"{data_dir}news_chunked/\"\n",
    "dir_created = create_dir(chunked_dir, clobber=True)\n",
    "print(f\"Created {dir_created} directory to store chunked datafiles (clobbering previous version).\")\n",
    "\n",
    "print(f\"Chunking {OriginalCorpus} into chunks with {chunklines} entires.\")\n",
    "\n",
    "# Load each frame into memory and then process it\n",
    "n_tot = 0\n",
    "for i, frame in enumerate(df):\n",
    "    print(f\"Processing chunk #{i+1:03d}:\")\n",
    "    # Dump output before any cleaning processing\n",
    "    if (DEBUG):\n",
    "        fname2 = f\"{preclean_dir}precleaned_news_{i+1:03d}.csv\"\n",
    "        print(f\"Creating {fname2} (DEBUG) ... \")\n",
    "        frame.to_csv(fname2, index=False)\n",
    "\n",
    "    # Run code to re-match website classes to entries and remove non-English content\n",
    "    revised_frame = clean_Dataframe(frame[['domain','type','title','content']], websites_df)\n",
    "\n",
    "    # Dump output before string cleaning processing\n",
    "    if (DEBUG):\n",
    "        fname1 = f\"{preclean_dir}midcleaned_news_{i+1:03d}.csv\"\n",
    "        print(f\"Creating {fname1} (DEBUG) ... \")\n",
    "        revised_frame.to_csv(fname1, index=False)\n",
    "\n",
    "    n_entries = revised_frame.shape[0]\n",
    "    if (n_entries>0):\n",
    "        # Check for NaN here that sneak through, just in case\n",
    "        bad_df = revised_frame[revised_frame['1st_type'].isnull()]\n",
    "        bad_domains = bad_df.drop_duplicates(subset = [\"domain\"])\n",
    "        if bad_domains.shape[0] > 0:\n",
    "            print(\"Empty '1st_type' for following domains: \")\n",
    "            print(bad_domains['domain'])\n",
    "\n",
    "        # Reduce content to first 800 characters\n",
    "        revised_frame[\"content\"] = revised_frame[\"content\"].str[:800]\n",
    "        # clean all the article content\n",
    "        revised_frame['content'] = revised_frame.apply(content_cleaner, axis=1)\n",
    "        # clean all the title content\n",
    "        revised_frame['title'] = revised_frame.apply(title_cleaner, axis=1)\n",
    "\n",
    "        # Dump articles with context before stripping the stop words (and thus wiping context)\n",
    "        fname3 = f\"{fullcontext_dir}news_{i+1:03d}.csv\"\n",
    "        print(f\"Creating {fname3} ... \")\n",
    "        revised_frame.to_csv(fname3, index=False)\n",
    "\n",
    "        # clean all the article stopwords, remove punctuation, and convert to lowercase\n",
    "        revised_frame['content'] = revised_frame.apply(content_second_scrub, axis=1)\n",
    "        # clean all the title stopwords, remove punctuation, and convert to lowercase\n",
    "        revised_frame['title'] = revised_frame.apply(title_second_scrub, axis=1)\n",
    "\n",
    "        fname = f\"{chunked_dir}news_{i+1:03d}.csv\"\n",
    "        print(f\"Creating {fname} ... \", end='')\n",
    "        revised_frame.to_csv(fname, index=False)\n",
    "        n_tot = n_tot + n_entries\n",
    "        print(f\"{n_entries} entries exported post cleaning.\")\n",
    "    else:\n",
    "        print(\"no entries survived cleaning.\")\n",
    "    del revised_frame   # Release the memory (or mark it as releasable)\n",
    "n_files = i+1\n",
    "print(f\"- A total of {n_tot} entries exported across {n_files} datafiles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test reading a chunked news file\n",
    "\n",
    "Read just the first file and see what you can do with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAEH6wT6QL9r"
   },
   "outputs": [],
   "source": [
    "chunked_dir = \"FakeNewsCorpus/news_chunked/\"\n",
    "chunked_df = pd.read_csv(f\"{chunked_dir}/news_001.csv\")\n",
    "\n",
    "print(f\"Columns are: {chunked_df.columns}\")\n",
    "print(f\"There are {chunked_df.shape[0]} entries in this file.\")\n",
    "print (\"First entry:\")\n",
    "print (f\"Title: {chunked_df.get('title')[0]}\")\n",
    "print (f\"Content: {chunked_df.get('content')[0]}\")\n",
    "print (f\"1st Type: {chunked_df.get('1st_type')[0]}\")\n",
    "print (f\"2nd Type: {chunked_df.get('2nd_type')[0]}\")\n",
    "print (f\"3rd Type: {chunked_df.get('3rd_type')[0]}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DataInspection412.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "12f6268eea5eaf9f8522c8e6c3e7b3815cade26c9b1340631fb84fc54a3083ce"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
